{
  "version": "2",
  "templates": [
    {
      "type": "container",
      "title": "InvokeAI (GPU Optimized)",
      "description": "Local AI image generation with full NVIDIA GPU acceleration (CUDA).",
      "note": "Requires NVIDIA GPU + Docker Desktop GPU support enabled.",
      "categories": ["AI", "Image Generation", "GPU"],
      "platform": "linux",
      "logo": "",
      "image": "ghcr.io/invoke-ai/invokeai:latest",
      "ports": [
        {
          "container": 9090,
          "host": 9090
        }
      ],
      "volumes": [
        {
          "container": "/data",
          "bind": "invokeai_data",
          "readonly": false
        }
      ],
      "env": [
        {
          "name": "INVOCATION_MODE",
          "label": "Invocation Mode",
          "default": "web"
        }
      ],
      "restart_policy": "always",
      "privileged": false,
      "devices": [],
      "host": {
        "gpus": "all"
      }
    },

    {
      "type": "container",
      "title": "ComfyUI (GPU)",
      "description": "Node-based Stable Diffusion workflow UI with GPU acceleration.",
      "note": "Bind a models directory if you want to re-use weights.",
      "categories": ["AI", "Image Generation", "GPU"],
      "platform": "linux",
      "logo": "",
      "image": "ghcr.io/comfyanonymous/comfyui:latest",
      "ports": [
        {
          "container": 8188,
          "host": 8188
        }
      ],
      "volumes": [
        {
          "container": "/root/.cache/comfyui",
          "bind": "comfyui_data",
          "readonly": false
        }
      ],
      "env": [],
      "restart_policy": "always",
      "privileged": false,
      "devices": [],
      "host": {
        "gpus": "all"
      }
    },

    {
      "type": "container",
      "title": "Automatic1111 Stable Diffusion WebUI (GPU)",
      "description": "Classic Stable Diffusion WebUI with full GPU support.",
      "note": "For testing; consider InvokeAI/ComfyUI for long-term workflows.",
      "categories": ["AI", "Image Generation", "GPU"],
      "platform": "linux",
      "logo": "",
      "image": "ghcr.io/abraham-ai/stable-diffusion-webui:latest",
      "ports": [
        {
          "container": 7860,
          "host": 7860
        }
      ],
      "volumes": [
        {
          "container": "/data",
          "bind": "a1111_data",
          "readonly": false
        }
      ],
      "env": [
        {
          "name": "CLI_ARGS",
          "label": "Additional arguments",
          "default": "--xformers"
        }
      ],
      "restart_policy": "unless-stopped",
      "privileged": false,
      "devices": [],
      "host": {
        "gpus": "all"
      }
    },

    {
      "type": "container",
      "title": "Ollama (GPU LLM Runtime)",
      "description": "Local LLM runtime with GPU acceleration for models like Llama, Mistral, etc.",
      "note": "Use ollama CLI or compatible UIs to interact with models.",
      "categories": ["AI", "LLM", "GPU"],
      "platform": "linux",
      "logo": "",
      "image": "ollama/ollama:latest",
      "ports": [
        {
          "container": 11434,
          "host": 11434
        }
      ],
      "volumes": [
        {
          "container": "/root/.ollama",
          "bind": "ollama_data",
          "readonly": false
        }
      ],
      "env": [],
      "restart_policy": "always",
      "privileged": false,
      "devices": [],
      "host": {
        "gpus": "all"
      }
    },

    {
      "type": "container",
      "title": "text-generation-webui (GPU)",
      "description": "Web UI for running and experimenting with text-generation models on GPU.",
      "note": "Ideal for managing multiple GGUF or HF models behind a graphical interface.",
      "categories": ["AI", "LLM", "GPU"],
      "platform": "linux",
      "logo": "",
      "image": "ghcr.io/oobabooga/text-generation-webui:latest",
      "ports": [
        {
          "container": 7861,
          "host": 7861
        }
      ],
      "volumes": [
        {
          "container": "/data",
          "bind": "tgwui_data",
          "readonly": false
        }
      ],
      "env": [
        {
          "name": "CLI_ARGS",
          "label": "Additional arguments",
          "default": "--listen --xformers"
        }
      ],
      "restart_policy": "unless-stopped",
      "privileged": false,
      "devices": [],
      "host": {
        "gpus": "all"
      }
    },

    {
      "type": "container",
      "title": "Whisper (GPU Speech-to-Text)",
      "description": "GPU-accelerated OpenAI Whisper for transcription and translation.",
      "note": "Mount an audio directory to /audio for batch jobs.",
      "categories": ["AI", "Audio", "GPU"],
      "platform": "linux",
      "logo": "",
      "image": "ghcr.io/ggerganov/whisper.cpp:latest",
      "ports": [],
      "volumes": [
        {
          "container": "/audio",
          "bind": "whisper_audio",
          "readonly": false
        }
      ],
      "env": [],
      "restart_policy": "unless-stopped",
      "privileged": false,
      "devices": [],
      "host": {
        "gpus": "all"
      }
    },

    {
      "type": "container",
      "title": "Kohya SS (GPU LoRA Trainer)",
      "description": "GPU-accelerated training UI for LoRA and fine-tuning Stable Diffusion models.",
      "note": "Training is VRAM-intensive; best suited for your 3090 Ti.",
      "categories": ["AI", "Training", "GPU"],
      "platform": "linux",
      "logo": "",
      "image": "ghcr.io/bmaltais/kohya_ss:latest",
      "ports": [
        {
          "container": 7862,
          "host": 7862
        }
      ],
      "volumes": [
        {
          "container": "/workspace",
          "bind": "kohya_workspace",
          "readonly": false
        }
      ],
      "env": [],
      "restart_policy": "unless-stopped",
      "privileged": false,
      "devices": [],
      "host": {
        "gpus": "all"
      }
    }
  ]
}